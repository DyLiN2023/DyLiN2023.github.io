<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DyLiN: Making Light Field Networks Dynamic</title>
  <!-- Bootstrap -->
  <link href="./css/bootstrap-4.4.1.css" rel="stylesheet">
  <link href="./css/project.css" rel="stylesheet">
  <link rel="stylesheet" href="./css/font-awesome.min.css">
</head>

<!-- cover -->

<body>
  <section>
    <div class="jumbotron text-center mt-0">
        <div class="section logos" style="text-align:center">
          <IMG src="./images/cmu-wordmark-stacked-r.png" height="60" border="0">
          </td>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;
          <IMG src="./images/RI_small.png" height="75"
              border="0"></td>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;
          <IMG src="./images/fujitsu.png" height="60" border="0"></td>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;
          <IMG src="./images/cvpr_logo.png" height="60" border="0">
          </td>
        </div></p>
        <div class="col-12">
          <h2>DyLiN: Making Light Field Networks Dynamic</h2>
          <h5><strong>(CVPR 2023)</strong></h5>            
          <!-- <h4 style="color:#5a6268;">Arxiv 2022</h4> -->
          <p></p>
          <h6>
          <div class="authors">
            <a href="https://heng14.github.io/" target="_blank">Heng Yu</a><sup> 1</sup>&#160;&#160;
            <a href="https://joeljulin.github.io/" target="_blank">Joel Julin</a><sup> 1</sup>&#160;&#160;
            <a href="https://scholar.google.com/citations?user=rSqodggAAAAJ&hl=es" target="_blank">Zoltán Á Milacski </a><sup> 1</sup>&#160;&#160;
            <a href="https://scholar.google.com/citations?user=AFaeUrYAAAAJ&hl=en" target="_blank">Koichiro Niinuma</a><sup> 2</sup>&#160;&#160;
            <a href="https://www.laszlojeni.com/" target="_blank">László A. Jeni</a><sup> 1</sup>&#160;&#160;
          </div>
          <div class="affiliations">
            <sup>1</sup>Carnegie Mellon University&#160;&#160;
            <sup>2</sup>Fujitsu Research of America&#160;&#160;
          </div>
          <br>
          <div class="row justify-content-center">
            <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org/abs/2303.14243" role="button" target="_blank">
                  <i class="fa fa-file"></i> Paper</a> </p>
            </div>
            <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/Heng14/DyLiN" role="button" target="_blank">
                <i class="fa fa-github"></i> Code</a> </p>
            </div>
          </div>
        </div>
    </div>
  </section>

  <section>
    <div class="jumbotron-grey">
      <div class="row">
        <div class="col-12 text-center">
          <h3><b>Abstract & Method</b></h3>
          <br>
          <div class="row" style="margin-bottom:5px">
            <div class="col" style="text-align:center">
              <img class="thumbnail" src="./images/front.png" style="width:80%; margin-bottom:20px">
            </div>
          </div>
          <p class="text-left">
            Light Field Networks, the re-formulations of radiance fields to oriented rays, are magnitudes faster than
            their coordinate network counterparts, and provide
            higher fidelity with respect to representing 3D structures from 2D observations. They would be well suited
            for generic scene representation and manipulation,
            but suffer from one problem: they are limited to holistic and static scenes. In this paper, we propose the
            Dynamic Light Field Network (DyLiN) method that can handle non-rigid deformations,
            including topological changes. We learn a deformation field from input rays to canonical rays, and lift them
            into a higher dimensional space to handle discontinuities.</p>
            <div class="row" style="margin-bottom:5px">
              <div class="col" style="text-align:center">
                <img class="thumbnail" src="./images/DyLiN.png" style="width:70%; margin-bottom:0px">
              </div>
            </div>
            <p class="text-fig">Schematic diagram of our proposed DyLiN architecture. Given a ray r = (o, d) and time t as input, we deform r into
              r' = (o', d'), and sample few points x<sub>k</sub>, k = 1, . . . , K along r' to
              encode it (blue). In parallel, we also lift r and t to the hyperspace
              code w (green), and concatenate it with each x<sub>k</sub>. We use the concatenation 
              to regress the RGB color of r at t directly (red).
            </p> 
            <p class="text-left">We further introduce CoDyLiN, which augments DyLiN with controllable
              attribute inputs. We train both models via knowledge distillation from pretrained dynamic 
              radiance fields. We evaluated DyLiN using both synthetic
              and real world datasets that include non-rigid deformations of
              varying difficulty and type. DyLiN qualitatively outperformed and quantitatively
              matched state-of-the art methods in terms of visual fidelity,
              while being 25−71× computationally faster. 
              We also tested CoDyLiN on attribute annotated data and it surpassed its teacher model.
            </p>
          </p>
        </div>
      </div>

    </div>
  </section>
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3><b>Video</b></h3>
          <iframe width="560" height="315" src="https://www.youtube.com/embed/8UGWWYJrv3s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
      </div>
    </div>
  </section>
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h3><b>Faster Rendering</b></h3>
          <p class="text-left"> Below demonstrates the rendering quality and speed of our DyLiN method (Right) compared to HyperNeRF
            (Left), the previous state-of-the-art, on real scenes.</p>
          <h6 class="text-center">&#128512; NOTE: The Left video is not a still image; it is just moving at 0.34 FPS, while ours renders at
            a smooth(er) 8.6 FPS!&#128512; </h6>
          <div class="video-container">
            <div class="embed-responsive embed-responsive-video-16by9">
              <video width="640" height="360" autoplay loop muted>
                <source src="./images/americano.mp4" type="video/mp4">
                <source src="./images/americano.webm" type="video/webm">
                Your browser does not support the video tag.
              </video>
            </div>
            <div class="embed-responsive embed-responsive-video-16by9">
              <video width="640" height="360" autoplay loop muted>
                <source src="./images/vrigchicken.mp4" type="video/mp4">
                <source src="./images/vrigchicken.webm" type="video/webm">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h3><b>Increased Visual Clarity</h3><b></b></h3>
          <p class="text-left"> We also tested DyLiN on synthetic scenes. In addition to significantly faster render times,
            we achieve superior fidelity! Results are played at the same FPS with the actual speed noted above.</p>
          <div class="embed-responsive embed-responsive-video-synthetic">
            <video width="640" height="360" autoplay loop muted>
              <source src="./images/lego-dylin.mov" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="embed-responsive embed-responsive-video-synthetic">
            <video width="640" height="360" autoplay loop muted>
              <source src="./images/jumping-dylin.mov" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="embed-responsive embed-responsive-video-synthetic">
            <video width="640" height="360" autoplay loop muted>
              <source src="./images/hook-dylin.mov" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h3><b>Controllable Extension</h3></b></h3>
          <p class="text-left"> To get the CoDyLiN architecture, we directly added masks to signify which regions of the scene are controllable and 
            attribute values for the state of the transition, with -1 being the start of the transition and +1 being the end. With the addition of both of these elements to DyLiN, 
            we can fully control the smile, closing of the eye, and raising of the eyebrow</p>
          <div class="embed-responsive embed-responsive-video-synthetic">
            <video width="640" height="360" autoplay loop muted>
              <source src="./images/codylin-control.mov" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <br>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section>
    <div class="container">
      <div class="row ">
        <div class="col-12">
            <h3><b>Citation</b></h3>
                <pre style="background-color: #e9eeef">
                  <code>
    @article{yu2023dylin,
      title={DyLiN: Making Light Field Networks Dynamic},
      author={Yu, Heng and Julin, Joel and Milacski, Zoltan A and Niinuma, Koichiro and Jeni, Laszlo A},
      journal={arXiv preprint arXiv:2303.14243},
      year={2023}
    }
                </code></pre>
        </div>
      </div>
    </div>
  </section>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h3><b>Acknowledgements</h3></b></h3>
          <p class="text-left">This research was supported partially by Fujitsu. We thank Chaoyang Wang from Carnegie Mellon University for the helpful discussion.</p>
          <br>
          </div>
        </div>
      </div>
    </div>
  </section>
  </div>


  </div>
</body>

</html>