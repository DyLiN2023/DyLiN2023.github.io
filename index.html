<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>CoNFies: Controllable Neural Face Avatars</title>
    <!-- Bootstrap -->
    <link href="./css/bootstrap-4.4.1.css" rel="stylesheet">
	<link href="./css/project.css" rel="stylesheet">
    <link rel="stylesheet" href="./css/font-awesome.min.css">
  </head>
  
  <!-- cover -->
  <body><section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
	  <!-- <div class="section logos" style="text-align:center">
      <IMG src="./images/cmu-wordmark-stacked-r.png" height="80" border="0">
      </td>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;
      <IMG src="./images/RI_small.jpeg" height="80"
          border="0"></td>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;
      <IMG src="./images/fujitsu.png" height="60" border="0"></td>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;
      <IMG src="./images/fg-logo.png" height="80" border="0">
      </td>
    </div> -->
  </p>
          <div class="col-12">
            <h2>DyLiN: Making Light Field Networks Dynamic
            </h2>
            <h2>(Paper ID 8412)
            </h2>            
            <!-- <h4 style="color:#5a6268;">Arxiv 2022</h4> -->
            <hr>
            <h6>
                <div class="authors">
          <a>Anonymous CVPR submission</a>&#160;&#160;
					<!-- <a href="https://heng14.github.io/" target="_blank">Heng Yu</a><sup> 1</sup>&#160;&#160;
					<a href="https://scholar.google.com/citations?user=AFaeUrYAAAAJ&hl=en" target="_blank">Koichiro Niinuma</a><sup> 2</sup>&#160;&#160;
					<a href="https://www.laszlojeni.com/" target="_blank">László A. Jeni</a><sup> 1</sup>&#160;&#160; -->
				</div>

				<!-- <div class="affiliations">
					<sup>1</sup>Carnegie Mellon University&#160;&#160;
					<sup>2</sup>Fujitsu Research of America&#160;&#160;
				</div> -->
                <!-- <p></p> -->
            <!-- <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org/abs/2211.08610" role="button" target="_blank">
                    <i class="fa fa-file"></i> Paper</a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://www.youtube.com/watch?v=DLgfEofnaoA" role="button" target="_blank">
                    <i class="fa fa-link"></i> Demo</a> </p>
              </div>
            </div> -->

          </h6></div>
        </div>
      </div>
    
  </section>


  <!-- abstract -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px">
            
            <div class="row" style="margin-bottom:5px">
              <div class="col" style="text-align:center">
                <img class="thumbnail" src="./images/pipeline_new_2.png" style="width:90%; margin-bottom:20px">
    
              </div>
    
            </div>
			
          <p class="text-left">
            Neural Radiance Fields (NeRF) are compelling tech- niques for modeling dynamic 3D scenes from 2D image collections. 
            These volumetric representations would be well suited for synthesizing novel facial expressions but for two problems. 
            First, deformable NeRFs are object agnostic and model holistic movement of the scene: they can replay how the motion changes 
            over time, but they cannot alter it in an interpretable way. Second, control- lable volumetric representations typically require 
            either time-consuming manual annotations or 3D supervision to provide semantic meaning to the scene. We propose a controllable 
            neural representation for face self-portraits (CoNFies), that solves both of these problems within a common framework, and it 
            can rely on automated processing. We use automated facial action recognition (AFAR) to characterize facial expressions as a 
            combi- nation of action units (AU) and their intensities. AUs provide both the semantic locations and control labels for the 
            system. CoNFies outperformed competing methods for novel view and expression synthesis in terms of visual and anatomic fidelity
            of expressions.
		  </p>
        </div>
      </div>
    </div>
  </section> -->
  <br>

  <!-- video -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Video</h3>
            <hr style="margin-top:0px">
            <div class="embed-responsive embed-responsive-16by9">
              <iframe width="640" height="360" src="./images/FG2023.mp4" allow="autoplay; encrypted-media" allowfullscreen></iframe>
			</div>
        </div>
      </div>
    </div>
  </section> -->

  <br>

  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Localized Control Results</h3>
            <hr style="margin-top:0px">
            <center>
				<ul>

          <div class="row">
            <div class="column">
              <img src="./images/americano.mp4" alt="local1" style="width:70%"><br/>Control eye opening (Left: CoNeRF. Right: ours.)</a><br>
            </div>
            <div class="column">
              <img src="./images/vrigchicken.mp4" alt="local2" style="width:70%"><br/>Control mouth opening (Left: CoNeRF. Right: ours.)</a><br>
            </div>
          </div> -->


					<!-- <li class="grid">
						<div class="griditem">
						<img src = "./images/decouplemask_attr_0.gif"><br/>Source</a><br>
						</div>
					</li>
					<li class="grid">
						<div class="griditem">
						<img src = "./images/decouplemask_attr_1.gif"><br/>Orange flowers</a><br>
						</div>
					</li> -->
					<!-- <li class="grid">
						<div class="griditem">
						<img src = "./images/1-3.gif"><br/>Yellow flowers</a><br>
						</div>
					</li>
					<li class="grid">
						<div class="griditem">
						<img src = "./images/1-4.gif"><br/>Yellow flowers with green leaves</a><br>
						</div>
					</li> -->
				<!-- </ul>
				<ul>
					<li class="grid">
						<div class="griditem2">
						<img src = "./images/2-1.gif"><br/>Source</a><br>
						</div>
					</li>
					<li class="grid">
						<div class="griditem2">
						<img src = "./images/2-2.gif"><br/>Red excavator</a><br>
						</div>
					</li> -->
					<!-- <li class="grid">
						<div class="griditem2">
						<img src = "./images/2-3.gif"><br/>Green excavator</a><br>
						</div>
					</li>
					<li class="grid">
						<div class="griditem2">
						<img src = "./images/2-4.gif"><br/>Pink excavator</a><br>
						</div> -->

					<!-- </li>
				</ul>
				</center>
        </div>
      </div>
    </div>
  </section> -->
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Demo Video</h3>
            <hr style="margin-top:0px"><br/>Left: HyperNeRF. Right: ours.</a><br>
            <div class="embed-responsive embed-responsive-16by9">
              <iframe width="640" height="360" src="./images/americano.mp4" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      			</div>
            <div class="embed-responsive embed-responsive-16by9">
              <iframe width="640" height="360" src="./images/vrigchicken.mp4" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      			</div>
            <!-- <div class="embed-responsive embed-responsive-16by9">
              <iframe width="640" height="360" src="./images/change_view.mp4" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      			</div>
            <div class="embed-responsive embed-responsive-16by9">
              <iframe width="640" height="360" src="./images/change_exp_view.mp4" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      			</div>
            <div class="embed-responsive embed-responsive-16by9">
              <iframe width="640" height="360" src="./images/exp_trans.mp4" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      			</div> -->
        </div>
      </div>
    </div>
  </section>



<br>
  <!-- citing -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em"><code>
@article{yu2022confies,
  title={CoNFies: Controllable Neural Face Avatars},
  author={Yu, Heng and Niinuma, Koichiro and Jeni, Laszlo A},
  journal={arXiv preprint arXiv:2211.08610},
  year={2022}
}
</code></pre>
          <hr>
      </div>
    </div>
  </div> -->

  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Acknowledgements</h3>
          <hr style="margin-top:0px">
          This research was supported by Fujitsu. We thank Joel Julin from University of Pittsburgh for helping with data collection 
          and comments that greatly improved the manuscript. We thank Xuxin Cheng from Carnegie Mellon University who provided the data 
          acquisition equipment. We would also like to show our gratitude to Nian-Hsuan Tsai from Carnegie Mellon University who helped 
          build the OpenFace system.
          <hr>
      </div>
    </div>
  </div> -->


    </div></body></html>