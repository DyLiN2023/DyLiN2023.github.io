<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DyLiN: Making Light Field Networks Dynamic</title>
  <!-- Bootstrap -->
  <link href="./css/bootstrap-4.4.1.css" rel="stylesheet">
  <link href="./css/project.css" rel="stylesheet">
  <link rel="stylesheet" href="./css/font-awesome.min.css">
</head>

<!-- cover -->

<body>
  <section>
    <div class="jumbotron text-center mt-0">
        <div class="section logos" style="text-align:center">
          <IMG src="./images/cmu-wordmark-stacked-r.png" height="80" border="0">
          </td>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;
          <IMG src="./images/RI_small.png" height="80"
              border="0"></td>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;
          <IMG src="./images/fujitsu.png" height="60" border="0"></td>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;
          <IMG src="./images/cvpr_logo.png" height="80" border="0">
          </td>
        </div></p>
        <div class="col-12">
          <h2>DyLiN: Making Light Field Networks Dynamic</h2>
          <h5><strong>(CVPR 2023 Demo)</strong></h5>            
          <!-- <h4 style="color:#5a6268;">Arxiv 2022</h4> -->
          <p></p>
          <h6>
          <div class="authors">
            <a href="https://heng14.github.io/" target="_blank">Heng Yu</a><sup> 1</sup>&#160;&#160;
            <a href="https://joeljulin.github.io/" target="_blank">Joel Julin</a><sup> 1</sup>&#160;&#160;
            <a href="https://scholar.google.com/citations?user=rSqodggAAAAJ&hl=es" target="_blank">Zoltán Á Milacski </a><sup> 1</sup>&#160;&#160;
            <a href="https://scholar.google.com/citations?user=AFaeUrYAAAAJ&hl=en" target="_blank">Koichiro Niinuma</a><sup> 2</sup>&#160;&#160;
            <a href="https://www.laszlojeni.com/" target="_blank">László A. Jeni</a><sup> 1</sup>&#160;&#160;
          </div>
          <div class="affiliations">
            <sup>1</sup>Carnegie Mellon University&#160;&#160;
            <sup>2</sup>Fujitsu Research of America&#160;&#160;
          </div>
          <!-- <div class="row justify-content-center">
            <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org/abs/2211.08610" role="button" target="_blank">
                  <i class="fa fa-file"></i> Paper</a> </p>
            </div>
          </div> -->
        </h6></div>
    </div>
  </section>

  <section>
    <div class="jumbotron-grey">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract & Method</h3>
          <br>
          <div class="row" style="margin-bottom:5px">
            <div class="col" style="text-align:center">
              <img class="thumbnail" src="./images/front.png" style="width:80%; margin-bottom:20px">
            </div>
          </div>
          <p class="text-left">
            Light Field Networks, the re-formulations of radiance fields to oriented rays, are magnitudes faster than
            their coordinate network counterparts, and provide
            higher fidelity with respect to representing 3D structures from 2D observations. They would be well suited
            for generic scene representation and manipulation,
            but suffer from one problem: they are limited to holistic and static scenes. In this paper, we propose the
            Dynamic Light Field Network (DyLiN) method that can handle non-rigid deformations,
            including topological changes. We learn a deformation field from input rays to canonical rays, and lift them
            into a higher dimensional space to handle discontinuities.</p>
            <div class="row" style="margin-bottom:5px">
              <div class="col" style="text-align:center">
                <img class="thumbnail" src="./images/DyLiN.png" style="width:70%; margin-bottom:0px">
              </div>
            </div>
            <p class="text-fig">Schematic diagram of our proposed DyLiN architecture. Given a ray r = (o, d) and time t as input, we deform r into
              r' = (o', d'), and sample few points x<sub>k</sub>, k = 1, . . . , K along r' to
              encode it (blue). In parallel, we also lift r and t to the hyperspace
              code w (green), and concatenate it with each x<sub>k</sub>. We use the concatenation 
              to regress the RGB color of r at t directly (red).
            </p> 
            <p class="text-left">We further introduce CoDyLiN, which augments DyLiN with controllable
              attribute inputs. We train both models via knowledge distillation from pretrained dynamic 
              radiance fields. We evaluated DyLiN using both synthetic
              and real world datasets that include non-rigid deformations of
              varying difficulty and type. DyLiN outperformed state-of-the art methods in terms of visual fidelity (1.4 −
              2.8dB average PSNR improvement) and compute complexity (25 − 71× render time speedup).
              We also tested CoDyLiN on attribute annotated data and it surpassed its teacher model.
            </p>
          </p>
        </div>
      </div>
      <!-- <div class="row">
        <div class="col-12 text-center">
          <h3>Method</h3>
          <br>
          <div class="row" style="margin-bottom:5px">
            <div class="col" style="text-align:center">
              <img class="thumbnail" src="./images/DyLiN.png" style="width:70%; margin-bottom:20px">
            </div>
          </div>
          <p class="text-left">
            
          </p>
        </div>
      </div> -->
    </div>
  </section>
  <br>
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Results</h3>
          <br>
          <p class="text-left"> Below demonstrates the rendering quality and speed of our DyLiN method (Right) compared to HyperNeRF
            (Left), the previous state-of-the-art, on real scenes.</p>
          <h6>&#128512; NOTE: The Left video is not a still image; it is just moving at 0.34 FPS, while ours renders at
            a smooth(er) 8.6 FPS!&#128512; </h6>
          <br>
          <div class="video-container">
            <div class="embed-responsive embed-responsive-video-16by9">
              <video width="640" height="360" autoplay loop muted>
                <source src="./images/americano.mp4" type="video/mp4">
                <source src="./images/americano.webm" type="video/webm">
                Your browser does not support the video tag.
              </video>
            </div>
            <div class="embed-responsive embed-responsive-video-16by9">
              <video width="640" height="360" autoplay loop muted>
                <source src="./images/vrigchicken.mp4" type="video/mp4">
                <source src="./images/vrigchicken.webm" type="video/webm">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
          <br>
          <br>
          <p class="text-left">We also tested DyLiN on synthetic scenes. In addition to significantly faster render times, we achieve
            superior fidelity! Ours-1 and Ours-2 were trained without and with fine-tuning on the original data, respectively.
          </p>
          <div class="row" style="margin-bottom:5px">
            <div class="col" style="text-align:center">
              <img class="thumbnail" src="./images/synthetic.png" style="width:90%; margin-bottom:20px">
            </div>
          </div>
          <br>
          <h5>More videos are on the way! Videos will demonstrate our CoDyLiN method, an extension of DyLiN to the controllable scenario.</h5>
        </div>
      </div>
    </div>
  </section>
  <br>

  </div>


  </div>
</body>

</html>